correlation coefficient
- The correlation coefficient is an informative summary of how two variables move together that can be used to predict one variable using the other.
- The correlation coefficient is defined for a list of pairs (x1,y1),...,(xn,yn) as the product of the standardized values: rho = ((xi−μx)/σx)*((yi−μy)/σy)
- The correlation coefficient essentially conveys how two variables move together.
- The correlation coefficient is always between -1 and 1.
- rho <- mean(scale(x)*scale(y))
- galton_heights %>% summarize(r = cor(father, son)) %>% pull(r)

- The correlation that we compute and use as a summary is a random variable.
- When interpreting correlations, it is important to remember that correlations derived from samples are estimates containing uncertainty.
- Because the sample correlation is an average of independent draws, the central limit theorem applies.

Anscombe's Quartet/Stratification
- Correlation is not always a good summary of the relationship between two variables.
- The general idea of conditional expectation is that we stratify a population into groups and compute summaries in each group.
- A practical way to improve the estimates of the conditional expectations is to define strata of with similar values of x.
- If there is perfect correlation, the regression line predicts an increase that is the same number of SDs for both variables. If there is 0 correlation, then we don’t use x at all for the prediction and simply predict the average μy. For values between 0 and 1, the prediction is somewhere in between. If the correlation is negative, we predict a reduction instead of an increase.

- Stratifying --> Conditional avarage

- When a pair of random variables are approximated by the bivariate normal distribution, scatterplots look like ovals. They can be thin (high correlation) or circle-shaped (no correlation).
- When two variables follow a bivariate normal distribution, computing the regression line is equivalent to computing conditional expectations.
- We can obtain a much more stable estimate of the conditional expectation by finding the regression line and using it to make predictions.

- Conditioning on a random variable X can help to reduce variance of response variable Y.
- The standard deviation of the conditional distribution is SD(Y|X=x)=σy*sqrt(1−ρ2), which is smaller than the standard deviation without conditioning  σy .
- Because variance is the standard deviation squared, the variance of the conditional distribution is σ2y(1−ρ2) .
- In the statement "X explains such and such percent of the variability," the percent value refers to the variance. The variance decreases by ρ2 percent.
- The “variance explained” statement only makes sense when the data is approximated by a bivariate normal distribution.
- There are two different regression lines depending on whether we are taking the expectation of Y given X or taking the expectation of X given Y.

Linear Models
- “Linear” here does not refer to lines, but rather to the fact that the conditional expectation is a linear combination of known quantities.
-  Galton's model, we assume Y (son's height) is a linear combination of a constant and X (father's height) plus random noise. We further assume that ϵi are independent from each other, have expected value 0 and the standard deviation σ which does not depend on i.
- Note that if we further assume that ϵ is normally distributed, then the model is exactly the same one we derived earlier by assuming bivariate normal data.
- We can subtract the mean from X to make β0 more interpretable. 

- For regression, we aim to find the coefficient values that minimize the distance of the fitted model to the data.
- Residual sum of squares (RSS) measures the distance between the true value and the predicted value given by the regression line. The values that minimize the RSS are called the least squares estimates (LSE).
- We can use partial derivatives to get the values for β0 and β1 in Galton's data.
- When calling the lm() function, the variable that we want to predict is put to the left of the ~ symbol, and the variables that we use to predict is put to the right of the ~ symbol. The intercept is added automatically.
- LSEs are random variables.
-- # fit regression line to predict son's height from father's height
-- fit <- lm(son ~ father, data = galton_heights)